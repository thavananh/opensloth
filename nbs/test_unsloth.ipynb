{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "171d506f-0422-4160-94e3-556d450f77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05b3046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "# Install latest Hugging Face for Gemma-3!\n",
    "!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d11a5bf7-0a4e-440d-87ba-ecbe8d441959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.14: Fast Qwen2 patching. Transformers: 4.50.0.dev0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.643 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca0580a5bca4d2eaae2b53db719615e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/202k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e5a515fe0d46b19ecdf27dcd65a643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a7f45f5fa54b75b1dbf3b3ce823ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4cd08a30fa440aab6de2608cf200dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.73G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00c416c9ac54a13b9778617ef360a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbed19b03a64f419e66178653ca1900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f29b203ad7248a6ac4054eb5be0d833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7765426b8740d19814bb2995f60fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d8b767e93648b889492dcf08d991a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa838b40e4e4a0ba36b05c3e7e78e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2439dc580fee44b8bef836c57634c1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef4912ee14d4b8fab3a7e8d9935619b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567e0d2d41204a0991009687e9e52b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # Choose any GPU you want to use\n",
    "import torch\n",
    "from unsloth import FastModel\n",
    "\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "\n",
    "    # Other popular models!\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/Llama-3.3-70B\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "    \"unsloth/Phi-4\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-14B-Instruct\",\n",
    "    max_seq_length = 16_000, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b076f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f5803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a46c7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24430/2684331598.py:5: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth.chat_templates import standardize_data_formats\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No backend type associated with device type cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeedy_utils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mall\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_templates\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m standardize_data_formats\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_chat_dataset\u001b[39m(\n\u001b[32m     10\u001b[39m     dataset_name: \u001b[38;5;28mstr\u001b[39m, split: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m, num_samples: \u001b[38;5;28mint\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, tokenizer: Any = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     11\u001b[39m ) -> Any:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/training/lib/python3.12/site-packages/unsloth/__init__.py:219\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Please install unsloth_zoo via `pip install unsloth_zoo`\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msave\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/training/lib/python3.12/site-packages/unsloth/models/__init__.py:15\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllama\u001b[39;00m\u001b[38;5;250m   \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLlamaModel\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[38;5;250m  \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel, FastVisionModel, FastTextModel, FastModel\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmistral\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastMistralModel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/training/lib/python3.12/site-packages/unsloth/models/llama.py:2752\u001b[39m\n\u001b[32m   2749\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   2751\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PatchFastRL\n\u001b[32m-> \u001b[39m\u001b[32m2752\u001b[39m \u001b[43mPatchFastRL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFastLanguageModel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mFastLlamaModel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/training/lib/python3.12/site-packages/unsloth/models/rl.py:741\u001b[39m, in \u001b[36mPatchFastRL\u001b[39m\u001b[34m(algorithm, FastLanguageModel)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mPatchFastRL\u001b[39m(algorithm = \u001b[38;5;28;01mNone\u001b[39;00m, FastLanguageModel = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    740\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m FastLanguageModel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: PatchRL(FastLanguageModel)\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m     \u001b[43mpatch_trl_rl_trainers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(algorithm) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m algorithm.islower():\n\u001b[32m    743\u001b[39m         PatchRLStatistics(algorithm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/training/lib/python3.12/site-packages/unsloth/models/rl.py:734\u001b[39m, in \u001b[36mpatch_trl_rl_trainers\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    732\u001b[39m all_trainers = [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_trainers \u001b[38;5;28;01mif\u001b[39;00m x.islower() \u001b[38;5;129;01mand\u001b[39;00m x.endswith(\u001b[33m\"\u001b[39m\u001b[33m_trainer\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m trainer \u001b[38;5;129;01min\u001b[39;00m all_trainers:\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m     \u001b[43m_patch_trl_rl_trainers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/training/lib/python3.12/site-packages/unsloth/models/rl.py:554\u001b[39m, in \u001b[36m_patch_trl_rl_trainers\u001b[39m\u001b[34m(trainer_file)\u001b[39m\n\u001b[32m    551\u001b[39m RLTrainer_source = re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mn]\u001b[39m\u001b[33m{\u001b[39m\u001b[33m3,}\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, RLTrainer_source)\n\u001b[32m    553\u001b[39m \u001b[38;5;66;03m# Create new function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m created_module = \u001b[43mcreate_new_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUnsloth\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mRLTrainer_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mRLTrainer_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrl.trainer.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtrainer_file\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimports\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# Patch Trainer\u001b[39;00m\n\u001b[32m    563\u001b[39m exec(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrl.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRLTrainer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m = created_module.Unsloth\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRLTrainer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/training/lib/python3.12/site-packages/unsloth_zoo/compiler.py:326\u001b[39m, in \u001b[36mcreate_new_function\u001b[39m\u001b[34m(name, new_source, model_location, functions, prepend, append, overwrite, add_torch_compile)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m UNSLOTH_COMPILE_USE_TEMP\n\u001b[32m    325\u001b[39m file_source = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m compile_folder, UNSLOTH_COMPILE_USE_TEMP = \u001b[43mget_compile_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tempfile\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m function_location = os.path.join(compile_folder, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# Check if file was already created!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/training/lib/python3.12/site-packages/unsloth_zoo/compiler.py:262\u001b[39m, in \u001b[36mget_compile_folder\u001b[39m\u001b[34m(use_tempfile)\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_compile_folder\u001b[39m(use_tempfile = \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     location, UNSLOTH_COMPILE_USE_TEMP = \u001b[43mdistributed_function\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_compile_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tempfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m location, UNSLOTH_COMPILE_USE_TEMP\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/training/lib/python3.12/site-packages/unsloth_zoo/utils.py:71\u001b[39m, in \u001b[36mdistributed_function\u001b[39m\u001b[34m(n, function, *args, **kwargs)\u001b[39m\n\u001b[32m     69\u001b[39m         object_list = [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# broadcast_object_list auto blocks so no need for barrier\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistributed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_object_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n == \u001b[32m1\u001b[39m: result = object_list[\u001b[32m0\u001b[39m]\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/training/lib/python3.12/site-packages/torch/distributed/c10d_logger.py:83\u001b[39m, in \u001b[36m_exception_logger.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m     85\u001b[39m         msg_dict = _get_msg_dict(func.\u001b[34m__name__\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/training/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:3129\u001b[39m, in \u001b[36mbroadcast_object_list\u001b[39m\u001b[34m(object_list, src, group, device)\u001b[39m\n\u001b[32m   3124\u001b[39m     object_sizes_tensor = torch.empty(\n\u001b[32m   3125\u001b[39m         \u001b[38;5;28mlen\u001b[39m(object_list), dtype=torch.long, device=current_device\n\u001b[32m   3126\u001b[39m     )\n\u001b[32m   3128\u001b[39m \u001b[38;5;66;03m# Broadcast object sizes\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3129\u001b[39m \u001b[43mbroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_sizes_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[38;5;66;03m# Concatenate and broadcast serialized object tensors\u001b[39;00m\n\u001b[32m   3132\u001b[39m \u001b[38;5;66;03m# Note: torch.cat will do an extra memory copy to the current device, if the tensor_list\u001b[39;00m\n\u001b[32m   3133\u001b[39m \u001b[38;5;66;03m# has only one element, we can skip the copy.\u001b[39;00m\n\u001b[32m   3134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m my_rank == src:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/training/lib/python3.12/site-packages/torch/distributed/c10d_logger.py:83\u001b[39m, in \u001b[36m_exception_logger.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m     85\u001b[39m         msg_dict = _get_msg_dict(func.\u001b[34m__name__\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/training/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:2417\u001b[39m, in \u001b[36mbroadcast\u001b[39m\u001b[34m(tensor, src, group, async_op)\u001b[39m\n\u001b[32m   2415\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m GroupMember.WORLD:\n\u001b[32m   2416\u001b[39m     default_pg = _get_default_group()\n\u001b[32m-> \u001b[39m\u001b[32m2417\u001b[39m     work = \u001b[43mdefault_pg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2419\u001b[39m     group_src_rank = get_group_rank(group, src)\n",
      "\u001b[31mRuntimeError\u001b[39m: No backend type associated with device type cpu"
     ]
    }
   ],
   "source": [
    "from pyexpat.errors import messages\n",
    "from typing import Any\n",
    "from speedy_utils.all import *\n",
    "from datasets import load_dataset\n",
    "from unsloth.chat_templates import standardize_data_formats\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def get_chat_dataset(\n",
    "    dataset_name: str, split: str = None, num_samples: int=None, tokenizer: Any = None\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): The name of the dataset to load.\n",
    "        split (str): The dataset split to load.\n",
    "        num_samples (int): The number of samples to select from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        Any: The preprocessed dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(dataset_name):\n",
    "        dataset = Dataset.from_json(dataset_name)\n",
    "    else:\n",
    "        dataset = load_dataset(dataset_name, split=split)\n",
    "    dataset = standardize_data_formats(dataset)\n",
    "\n",
    "    def apply_chat_template(examples):\n",
    "        messages_key = \"messages\" if \"messages\" in examples else \"conversations\"\n",
    "        texts = tokenizer.apply_chat_template(examples[messages_key], tokenize=False)\n",
    "        return {\"text\": texts}\n",
    "    if num_samples:\n",
    "        num_samples = min(num_samples, len(dataset))\n",
    "        dataset = dataset.shuffle(seed=42)\n",
    "        dataset = dataset.select(range(num_samples))\n",
    "    if tokenizer:\n",
    "        dataset = dataset.map(apply_chat_template, batched=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Example usage\n",
    "dataset = get_chat_dataset(\"./data/cod_1k.json\", num_samples=100*8, tokenizer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e693a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833a1bfad3f5485f8d0caffb62a94020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45acd11549194ca695b0dfe85dbac64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_num_proc=4,\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 8, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        # max_steps = 1000,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16cd842d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Example 8. Find the integral $\\int \\frac{d x}{\\sin x}$.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Find ‚à´ 1/sin x dx.\n",
      "\n",
      "Consider substitution as a method.\n",
      "\n",
      "1/sin x = csc x, but derive.\n",
      "\n",
      "Rewrite: (sin x)/sin¬≤x. Use u = cos x.\n",
      "\n",
      "Let u = cos x, du = -sin x dx.\n",
      "\n",
      "‚à´ sin x / (1 - cos¬≤x) dx = ‚à´ -du / (1 - u¬≤).\n",
      "\n",
      "‚à´ du/(1 - u¬≤) = (1/2) ln |(1+u)/(1-u)| + C.\n",
      "\n",
      "Substitute back u = cos x.\n",
      "\n",
      "-(1/2) ln |(1 + cos x)/(1 - cos x)| + C.\n",
      "\n",
      "Simplify, or check tan(x/2) form?\n",
      "\n",
      "Simplify the expression.\n",
      "\n",
      "-(1/2) ln |(1 + cos x)/(1 - cos x)|\n",
      "\n",
      "Multiply by (1 + cos x)/(1 + cos x).\n",
      "\n",
      "[(1 + cos x)¬≤]/sin¬≤x\n",
      "\n",
      "(1 + cos x)/(1 - cos x) = [(1 + cos x)/sin x]¬≤.\n",
      "\n",
      "Log of square simplifies it.\n",
      "\n",
      "ln[(1 + cos x)/(1 - cos x)] = 2 ln [(1 + cos x)/sin x].\n",
      "\n",
      "ln [ (1 + cos x)/(1 - cos x) ] = 2 ln [ (1 + cos x)/sin x ]\n",
      "\n",
      "Express in terms of tan(x/2)?\n",
      "\n",
      "1/tan(x/2) = (1 + cos x)/sin x; ln[(1 + cos x)/sin x] = -ln|tan(x/2)|.\n",
      "\n",
      "- (1/2) ln |(1 + cos x)/(1 - cos x)| =  - ln |(1 + cos x)/sin x|.\n",
      "\n",
      "Recall standard integral formula.\n",
      "\n",
      "‚à´ csc x dx = ln |tan(x/2)| + C.\n",
      "\n",
      "Check if expressions are equivalent.\n",
      "\n",
      "Start with -(1/2) ln |(1 + cos x)/(1 - cos x)|.\n",
      "\n",
      "Multiply inside log by (1+cos x).\n",
      "\n",
      "(1 + cos x)¬≤ / sin¬≤x.\n",
      "\n",
      "Rewrite: ln[(1+cos x)/sin x]  = -ln|tan(x/2)|.\n",
      "\n",
      "Original expression simplifies.\n",
      "\n",
      "-(1/2) ln |(1 + cos x)/(1 - cos x)| = - ln |(1 + cos x)/sin x|\n",
      "\n",
      "Relate ln|tan(x/2)| to other functions.\n",
      "\n",
      "Start from -(1/2) ln |(1 + cos x)/(1 - cos x)|.\n",
      "\n",
      "Factor out the negative sign.\n",
      "\n",
      "(1/2) ln |(1 - cos x)/(1 + cos x)|\n",
      "\n",
      "1 - cos x = 2sin¬≤(x/2), 1 + cos x = 2cos¬≤(x/2).\n",
      "\n",
      "(1 - cos x)/(1 + cos x) = tan¬≤(x/2). Result: ln |tan(x/2)|.\n",
      "\n",
      "Integral is ln|tan(x/2)|+C.\n",
      "\n",
      "Recall the expression.\n",
      "\n",
      "(1/2) ln |(1 - cos x)/(1 + cos x)| = ln |tan(x/2)|.\n",
      "\n",
      "Integral is ln|tan(x/2)| + C.\n",
      "\n",
      "Also equal to -ln|csc x + cot x|?\n",
      "\n",
      "‚à´ csc x dx = -ln |csc x + cot x| + C\n",
      "\n",
      "Compare ln |tan(x/2)| and -ln |csc x + cot x|.\n",
      "\n",
      "tan(x/2) = sin x/(1+cos x) then, ln |tan(x/2)| = ln |sin x| - ln |1 + cos x| , also -ln|(1+cos x)/sin x|.\n",
      "\n",
      "Both forms are equivalent.\n",
      "\n",
      "‚à´ dx/sin x = ln |tan(x/2)| + C OR -ln |csc x + cot x| + C.\n",
      "\n",
      "ln |tan(x/2)| + C is more simplified form.\n",
      "\n",
      "Verify using another method.\n",
      "\n",
      "Multiply by sin x / sin x.\n",
      "\n",
      "Same substitution.\n",
      "\n",
      "Use t = tan(x/2), Weierstrass substitution.\n",
      "\n",
      "sin x = 2t/(1 + t¬≤), dx = 2 dt/(1 + t¬≤).\n",
      "\n",
      "Integral simplifies to ln |t| + C = ln |tan(x/2)| + C.\n",
      "\n",
      "Integral is ln |tan(x/2)| + C (Confirmed).\n",
      "\n",
      "Other method: Use csc x.\n",
      "\n",
      "Summarize steps.\n",
      "\n",
      "1. ‚à´ [sin x dx]/[1 - cos¬≤x].\n",
      "\n",
      "2. u=cos x => ‚à´ -du/(1 - u¬≤).\n",
      "\n",
      "3. Integrate using partial fractions.\n",
      "\n",
      "4. Substitute back u = cos x.\n",
      "\n",
      "5. Simplify to ln\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(trainer.train_dataset[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a68de14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1ec160acb64393a21a6f7342671905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=96):   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|im_start|>user\",\n",
    "    response_part = \"<|im_start|>assistant\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc10dff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 800 | Num Epochs = 1 | Total steps = 25\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 8 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 34,406,400/14,000,000,000 (0.25% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 09:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.148300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.053300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.127100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.950800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.968300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.865700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.998400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.886400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.980800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.912300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.861200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.787100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.842600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.861000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.749500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.806300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.923200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.859300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.725200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.837900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cce39a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.643 GB.\n",
      "13.807 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e65d0973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llm_utils import get_conversation_one_turn\n",
    "\n",
    "\n",
    "# item = dataset[0]\n",
    "# # messages = item[\"messages\"][:-1]\n",
    "# messages = get_conversation_one_turn(None, 'ho√†ng sa c·ªßa n∆∞·ªõc n√†o, h√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng vi·ªát')\n",
    "\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt = True, # Must add for generation\n",
    "#     tokenize=False,\n",
    "# )\n",
    "# text += '<think>\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40e2b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from transformers import TextStreamer\n",
    "# with torch.inference_mode():\n",
    "#     _ = model.generate(\n",
    "#         **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
    "#         max_new_tokens = 6400, # Increase for longer outputs!\n",
    "#         # Recommended Gemma-3 settings!\n",
    "#         temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "#         streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e2cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22709fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading safetensors index for unsloth/qwen2.5-14b-instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6bd0fcf9cd4c6a88080bee170e501d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/47.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11aac0517b5c47feb5bcf6668f27efd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  17%|‚ñà‚ñã        | 1/6 [00:57<04:48, 57.71s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e2c1d4af1d40b8bd37e90e22bb237f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  33%|‚ñà‚ñà‚ñà‚ñé      | 2/6 [01:57<03:55, 58.76s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e489f3cbe70d4a52a540e598630c93b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [02:51<02:50, 56.77s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d14d468ecab49b1875bec5d1dc0b2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [03:47<01:52, 56.47s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec483abfcbca4e5faac19e10b2b40c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [04:44<00:56, 56.69s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de047392a13a483caf55c6e61a227334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/4.73G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [05:38<00:00, 56.36s/it]\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"mymodel\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e5d23",
   "metadata": {},
   "source": [
    "### VLLM INFerence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0020b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
